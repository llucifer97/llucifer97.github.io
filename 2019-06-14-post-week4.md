---
title: week3
categories:
- GSOC
- annotationTool
- Deployment
feature_image: "https://picsum.photos/2560/600?image=872"
---
This week I was a bit low on work, since I was able to complete the maximum task in the past week,most of time went into adding features and making the tool more interactive.I was finding a way to deploy the jupyter notebook,where I came across this website Binder[] which allows us to render our jupyter notebook and also make it available to run and edit the code without any credentials.Also,This tool if used by a non-technical person could be overwhelming in a sense that they have to deal with code ,run it every time they want to use the app.So,to remove this proproblem I did some research and came across some very cool extension called <b>appmode</b> which enables u to run the jupyter notebook in a app like view.
So,the basic construct of the making and deployment of the app was:
<ul>
    <li>Hosting the jupyter notebook on Github</li>
    <li>Use binder to render the notebook</li>
    <li>Use ipython widget to make the app</li>
    <li>use appmode extesion to make it app</li>
</ul>

Secondly, I have also deployed the the jupyter notebook on the singularity container, which required me to write singularity recepie , job schedueler and pull it on the gallina server.To learn how to deploy the singularity container u can visit my github project page.

Also,I had a conversation wit Leonard Impett sir about the future of the project and what are the imapct this project is will create in the people working closely related to art. My mentors did gave my project a new level. Starting with raw data and crudely annotating it we now reached to a position were we now have a working tool wich not only helps in visualising the gesture but also correct it.Mentors are really exited to help me and share knowledge,like lot of times I was not able to grasp the idea they talked about during the skype call, so they tried there level best to make there idea clear with me .This seems to be the completion of project only few more functionality is yet to be implemeted so that we can actually not only correct the gesture but could share the same , with amazon mechnanical turk to annotate the data and make the data set ready for my second phase of the project which is building the sequence based machine learning model to classify the gesture of medeival period.

Things to be implemented in upcoming week:
<ul>
  <li>write a api to get the processsed json file containing the keypoints generated by the GPU cluster at HPC</li>
  <li>another api to get the link of the image via iiif </li>
  <li>a database to store the incoming images keypoints</li>
  <li>make the system such that it is easily shareable with amazon mechnical turk for annotatio</li>
</ul>
 thank you, this is it .Will keep you posted!

